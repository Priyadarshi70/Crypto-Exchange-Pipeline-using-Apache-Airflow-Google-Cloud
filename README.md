
This project is a complete end-to-end data pipeline built using Apache Airflow, leveraging the CoinGecko API to fetch live cryptocurrency market data, transform it into a structured format, and store both raw and processed data in Google Cloud Storage (GCS).
The pipeline is automated, scalable, and designed to run every 10 minutes.

ğŸš€ Project Purpose

Cryptocurrency data changes rapidly, and having a reliable pipeline to automatically collect, process, and store this data is very valuable for analysis, reporting, or building dashboards.
This pipeline solves that by:

âœ” Fetching the latest crypto data
âœ” Transforming it into structured format (CSV)
âœ” Uploading both raw and processed files to GCS
âœ” Organizing data by timestamp for future use

ğŸ› ï¸ Tech Stack
Component	Technology
Orchestration	Apache Airflow
Data Source	CoinGecko API
Cloud Storage	Google Cloud Storage (GCS)
Programming Language	Python
Libraries Used	pandas, requests, datetime, json
Airflow Providers	apache-airflow-providers-google
ğŸ“‚ Project Folder Structure
crypto-exchange-pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ crypto_exchange_pipeline_corrected.py
â”‚
â”œâ”€â”€ crypto_data.json
â”œâ”€â”€ transformed_data.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”„ Pipeline Workflow
1ï¸âƒ£ Fetch Data from CoinGecko API  
2ï¸âƒ£ Save data locally as crypto_data.json  
3ï¸âƒ£ Create a GCS bucket (if not exists)  
4ï¸âƒ£ Upload raw JSON file to GCS  
5ï¸âƒ£ Transform JSON into structured CSV  
6ï¸âƒ£ Upload transformed CSV to GCS with timestamp

ğŸ“Š Fields Extracted from API

Each cryptocurrency record includes:

ID
Name
Symbol
Current Price
Market Cap
Total Volume
Last Updated Time

Pipeline timestamp (UTC format)

âš™ï¸ Google Cloud Configuration
GCP_PROJECT = "learn-airflow-428415"
GCS_BUCKET = "crypto-exchange-pipeline-priyadarshigupta"
GCS_RAW_DATA_PATH = "raw_data/crypto_raw_data"
GCS_TRANSFORMED_DATA_PATH = "transformed_data/crypto_raw_data"

â² Schedule

The DAG is configured to run every 10 minutes, ensuring near real-time data availability.

 DAG Visualization
fetch_data_task
        â†“
create_bucket_task
        â†“
upload_raw_data_to_gcs
        â†“
transform_data_task
        â†“
upload_transformed_data_to_gcs

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Install Required Libraries
pip install apache-airflow apache-airflow-providers-google pandas requests

2ï¸âƒ£ Place your DAG file in Airflow DAGs folder
/opt/airflow/dags/crypto_exchange_pipeline_corrected.py

3ï¸âƒ£ Initialize Airflow and Start Services
airflow db init
airflow webserver --port 8080
airflow scheduler

4ï¸âƒ£ Open Airflow UI and Activate the DAG

Open: http://localhost:8080

Search for: crypto_exchange_pipeline_corrected

Turn toggle ON

ğŸ“¤ Output Stored in Google Cloud

Raw Data:

gs://crypto-exchange-pipeline-priyadarshigupta/raw_data/crypto_raw_data/crypto_data.json


Transformed Data:

gs://crypto-exchange-pipeline-priyadarshigupta/transformed_data/crypto_raw_data/transformed_data_202502051005.csv

ğŸ§¾ Common Issues & Fixes
Issue	Reason	Fix
data=json.load(data,f)	Wrong syntax	Use data = json.load(f)
pd.dataframe error	Wrong function name	Use pd.DataFrame()
tra.json file not found	Incorrect filename	Replace with "transformed_data.csv"
Missing API keys	CoinGecko doesnâ€™t need keys	Works without credentials
Undefined fields in API	Fields not available	Use item.get('field_name')
ğŸŒŸ Future Improvements

ğŸ”¹ Load transformed data into BigQuery
ğŸ”¹ Add Spark for heavy data processing
ğŸ”¹ Build Tableau / Power BI dashboard
ğŸ”¹ Add Slack / Email alerts on DAG failure

ğŸ¤ Want to Contribute?

Feel free to fork this repository, make improvements, and raise pull requests.
All meaningful contributions are welcome!

ğŸ“ License

This project is licensed under the MIT License.

ğŸ™Œ Author

Priyadarshi Gupta
Data Engineering Enthusiast | Cloud & ETL Learner
